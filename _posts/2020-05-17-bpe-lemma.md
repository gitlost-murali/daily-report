---
toc: true
layout: post
description: Understanding the workings of BPE and intro to lemmatization
categories: [slp100]
title: BPE algo,  Lemmatization
---

# Byte Pair Encoding

Out of the 3 tokenization methods I mentioned last time (BPE, Unigram Language Modeling & WordPiece), BPE is the most common and easy to understand.

## Workings of BPE

As mentioned earlier, each tokenization scheme consists of two parts, 

1. Token learner
2. Token segmenter

### Token learner

Let's understand the first part i.e 1. Inducing vocabulary

BPE starts by segmenting the tokens from inside. To be specific, input given to BPE is a list of tokens split by white space and BPE learns sub-words based on the information inside each token.

Let's understand this with the following example,

For brevity, let's assume a tiny input corpus with few words. We split the corpus by white space and append a "_" for each token marking the word-ending. We then maintain a frequency of each word. This info is given to BPE algo.

> Input to BPE
```
Count   Token
5        l o w _
2        l o w e s t _
6        n e w e r _
3        w i l d e r _
2        n e w _
```
We are passing the tokens as list of characters.

We initialize the vocabulary with the set of uniq characters. As per this example, it will be 

`Vocabulary V -> d, e, i, l, n, o, r, s, t, w`

Now, we count all adjacent pairs across the training corpus and pick the one with high frequency.

Ex: 

For the first word `low_`, adjacent pairs are `lo`, `ow`, `w_` and freq of each it is 5 i.e freq of the word `low_`.
If we consider all words and update the freq counts of these adjacent pairs, we'd be getting

```
er: 9
ne: 8
lo: 7
..
..
```

Since `er` has high freq, we note this merge `(e,r)` and add the pair to vocabulary. We replace all adjacent pair instances of `e r` with `er`. Updated vocabulary is

`V = V + er`

Updated input is

```
Count   Token
5       l o w _
2       l o w e s t _
6       n e w er _
3       w i l d er _
2       n e w _
```

By repeating the same procedure, you'd find (er, _). And it goes on....

Finally, you'd be left with a vocabulary. `Vocabulary V -> d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer_, low_`.

### Token Segmenter

Once we have learned the vocabulary, the second part -> Token Segmenter. It will simply use the merges, in the order we learned, to split the test corpus accordingly.

Let's understand how Token Segmenter with an example

Consider that `lower` is the new word in test set:

Now, according to the order, our first merge is (e,r), so

l o w e r _ -> l o w __er__ _

Next, we find (er,_), so

l o w er _ ->  l o w **er_**

Next, we find (l,o), so

l o w er_ -> __lo__ w er_

Next, we find (lo, w), so

lo w er_ -> __low__ er_

We are finally left with two tokens for the token _lower_, low and er_.

## Lemmatization

