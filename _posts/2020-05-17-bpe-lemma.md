---
toc: true
layout: post
description: Understanding the workings of BPE and intro to lemmatization
categories: [slp100]
title: BPE algo,  Lemmatization
---

# Byte Pair Encoding & Lemmatization

I. Out of the 3 tokenization methods I mentioned last time (BPE, Unigram Language Modeling & WordPiece), BPE is the most common and easy to understand.
II. Lemmatization is often used in Information Extraction, Retrieval since search systems must behave similarly to morphological variants of the same word. (Cricketer, Cricket, Dinners, Dinner, etc.)  

## Workings of BPE

As mentioned earlier, each tokenization scheme consists of two parts, 

1. Token learner
2. Token segmenter

### Token learner

Let's understand the first part i.e 1. Inducing vocabulary

BPE starts by segmenting the tokens from inside. To be specific, input given to BPE is a list of tokens split by white space and BPE learns sub-words based on the information inside each token.

Let's understand this with the following example,

For brevity, let's assume a tiny input corpus with few words. We split the corpus by white space and append a "_" for each token marking the word-ending. We then maintain a frequency of each word. This info is given to BPE algo.

> Input to BPE
```
Count   Token
5        l o w _
2        l o w e s t _
6        n e w e r _
3        w i l d e r _
2        n e w _
```
We are passing the tokens as list of characters.

We initialize the vocabulary with the set of uniq characters. As per this example, it will be 

`Vocabulary V -> d, e, i, l, n, o, r, s, t, w`

Now, we count all adjacent pairs across the training corpus and pick the one with high frequency.

Ex: 

For the first word `low_`, adjacent pairs are `lo`, `ow`, `w_` and freq of each it is 5 i.e freq of the word `low_`.
If we consider all words and update the freq counts of these adjacent pairs, we'd be getting

```
er: 9
ne: 8
lo: 7
..
..
```

Since `er` has high freq, we note this merge `(e,r)` and add the pair to vocabulary. We replace all adjacent pair instances of `e r` with `er`. Updated vocabulary is

`V = V + er`

Updated input is

```
Count   Token
5       l o w _
2       l o w e s t _
6       n e w er _
3       w i l d er _
2       n e w _
```

By repeating the same procedure, you'd find (er, _). And it goes on....

Finally, you'd be left with a vocabulary. `Vocabulary V -> d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer_, low_`.

### Token Segmenter

Once we have learned the vocabulary, the second part -> Token Segmenter. It will simply use the merges, in the order we learned, to split the test corpus accordingly.

Let's understand how Token Segmenter with an example

Consider that `lower` is the new word in test set:

Now, according to the order, our first merge is (e,r), so

l o w e r _ -> l o w __er__ _

Next, we find (er,_), so

l o w er _ ->  l o w **er_**

Next, we find (l,o), so

l o w er_ -> __lo__ w er_

Next, we find (lo, w), so

lo w er_ -> __low__ er_

We are finally left with two tokens for the token _lower_, low and er_.

## Lemmatization

Lemmatization is the task of identifying whether two words come from the same root word. 
For ex, 

i) `is, are, am` belong to the root `be`. 

ii) `memories, memory` has the root word `memory`.

There are two ways to do lemmatization.

1. Morphological Parsing (Sophisticated & complex): Morphology is the study of how a word is formed by combined smaller meaning-bearer units often called Morphemes. Morphemes can be classified into two forms. a. __Stems__ b. __Affixes__.
    a. Stems: Central morphemes that supply the main meaning of the word
    b. Affixes: Additional part of the word.

    Ex: Cars -> Car + s

2. Stemming (Simple yet rudimentary): Stemming logic is mostly based on chopping off the affixes of a word. Porter-Stemmer is a rule-based system where input is processed through a series of steps (cascade).

Sample rules:

ATIONAL -> ATE (Relational -> Relate)
ING -> "" If Stem contains vowel (Monitoring -> Monitor)
SSES -> SS (Grasses -> Grass, Kisses -> Kiss)

Stemming is often prone to errors with the only advantage as being faster.

## Normalization

Normalization is the process of converting words to a standard format. This is useful in Information retrieval systems. For ex: US & USA represent the country United States. Wow, Wowww mean Wow.

Case-Folding: Lower casing the words has an upside of lowering the workload of NLP systems. For example, Boring and boring mean the same thing. Just storing `boring` would save space. However, this has exceptions.
1. It is difficult to differential Location `US` and Pronoun `us`, when lower-cased.
2. True-casing helps in understanding the emotion of text. For ex, Capitalized words would magnify the emotion of reviewer in a review. These features help the classifier.
And so on..