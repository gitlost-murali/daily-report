---
toc: true
layout: post
description: Formal Intro to Language Models, N-Gram LMs, Markov assumptions & Maximum Likelihood
categories: [slp100]
title: Language Models - N-Grams & Markov assumptions & Maximum Likelihood 
---

# Language Models

Briefly, Language Models, are `models` that assign probability to each possible next word given a sequence. LMs can also assign the probability to sentences. This helps in choosing which of the two sentences are more probable in occuring in the corpus.

LMs are used in grammar correction and speech recognition systems to select the appropriate phrase or sequence. Statistical LMs make use of N-grams for obtaining probabilities.

## N-Grams

N-Grams are sequence of n-words. Bi-grams, Tri-grams, etc.

## Predicting probability of next word

One way to predict the probability of next word is in the following way,

$$
P(w/h) = P(w âˆ© h)/ P(h)
$$

$$ 
P(the| BS\ is\ so\ great\ that) = \#(BS\ is\ so\ great\ that\ the)/\#(BS\ is\ so\ great\ that) 
$$

Here, we are using relative counts which are pre-calculated from the corpus/web. This method fails even for a slight change in the sentence. 

For ex, a new sentence __The Mombie is so great that__ may contain a new lingo like `Mombie`. Since it's a new word, the occurence of #(The Mombie is so great that) would be 0.

## Predicting probability of a sentence

Similarly, to calculate the probability of a sentence or a joint probability of sequence of words like __the country is so naive__, we could do,

$$ 
\#(the\ country\ is\ so\ naive) /\#(All\ possible\ five\ word\ order\ sequences) 
$$

Calculating the denominator i.e #(All possible five word order sequences) is a computational heavy process and is not recommended.

Instead, we could leverage chain rule of probability to solve this issue where,

$$ P(X_{1}, X_{2}, X_{3}, ... X_{n}) = P(X_{1})\ x\ P(X_{2}|X_{1})\ x\ P(X_{3}|X_{1}^{2})\ x\ ... P(X_{n}|X_{1}^{n-1}) $$

We can apply the same to words..

$$ P(w_{1}, w_{2}, w_{3}, ... w_{n}) = P(w_{1})\ x\ P(w_{2}|w_{1})\ x\ P(w_{3}|w_{1}^{2})\ x\ ... P(w_{n}|w_{1}^{n-1}) $$

Where 
$$
w_{1}^{n-1}
$$ 
means the string 
$$
w_{1} w_{1} .. w_{n-1}
$$

A sore thumb in this approach is calculating the probability 
$$
P(w_{n}|w_{1}^{n-1})
$$ since it is difficult to estimate it by counting the #(word occurs with its history), as language is creative (discussed above too - `Mombie`).

## Markov assumption

So, we approximate the long previous history with last _k_ words. Specifically, instead of calculating P(word given entire history), we just calculate P(word given last _k_ words). This is the ideology behing N-gram model.

Whenever we use bigram model to make a prediction, we make the following assumption,

$$P(w_{n}|w_{1}^{n-1}) \approx P(w_{n}|w_{n-1})$$

The above assumption is called Markov assumption. Markov models are a class of probabilistic models that make an assumption that we can predict the probability of a future event w/o looking too far into the past.

## Maximum Likelihood estimate

Probability P(word) can be estimated by Maximum Likelihood estimate. Broadly, Estimation of parameters from the existing data points is called Maximum Likelihood Estimation. In terms of distribution, the definition can modified to _Estimation of optimal mean, variance (parameters in this case) from the existing data points_. In terms of text, the definition would be estimation of probability from the existing corpus.

As per SLP book,
> We get MLE estimate for the parameters of an N-Gram model by getting counts from a corpus and normalizing the counts so that they lie between 0 and 1. For probabilistic models, normalizing means dividing by the total count so that probabilities always lie in the range of 0,1.


{% include note.html content="For better understanding of Maximum Likelihood, visit [StatQuest's video](https://www.youtube.com/watch?v=XepXtl9YKwc)
" %}

MLE estimate for a Bi-Gram model,
$$P(w_{n}|w_{1}^{n-1}) = \#(w_{n-1}\ w_{n})/ \#(w_{n-1}) $$

Incase of a N-gram, MLE estimate is 

$$P(w_{n}|w_{1}^{n-1}) = \#(w_{n-N+1} ... w_{n-1}\ w_{n})/ \#(w_{n-N+1}..w_{n-1}) $$

### More on Maximum Likelihood Estimate

Well, MLE is more of a daunting word for beginners. What we are essentially doing is a simple relative frequency => Out of some total counts, how many are suitable counts. That's what we are doing.

Utilizing this relative frequency as an estimate for probability is an example of Maximum Likelihood estimation. Whatever method you chose, the resulting optimal parameter set found for model M should maximize the likelihood of the training set T given M i.e $ P(T/M) $. We can think of it in different domains. For deep learning folks, this means whatever architecture you chose, your accuracy must be high for the training set.

## Concepts in practice

In this section, let's see how we can calculate the probability of a sentence.

Given a corpus, we can generate bigrams and their frequencies. For a sample corpus, these are the statistics:

<p><img src="{{ site.baseurl }}/images/lms/bigram-table-slp.png" alt="" title="Bigram counts from the sample corpus. Reference: https://web.stanford.edu/~jurafsky/slp3/"></p>

__i__ | __want__ | __to__ | __eat__ | __chinese__ | __food__ | __lunch__ | __spend__
2533 | 927 | 2417 | 746 | 158 | 1093 | 341 | 278

Frequencies of each word. Call them unigrams too. Now, let's normalize the counts which can then be directly used in MLE estimate.

<p><img src="{{ site.baseurl }}/images/lms/bigram-normalize-slp.png" alt="" title="Normalized Bigram counts. Reference: https://web.stanford.edu/~jurafsky/slp3/"></p>

Let's calculate the probability of a sentence with the above counts
> I want english food

$$ P(<s>\ I\ want\ english\ food\ </s>) = P(I|<s>) * P(want|I) * P(english|want) * P(food|want) * P(</s>|food) \newline \newline

P(<s>\ I\ want\ english\ food\ </s>) = 0.0031 $$