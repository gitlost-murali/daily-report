<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>BPE algo, Lemmatization | Murali Manohar Daily Tracker</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="BPE algo, Lemmatization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understanding the workings of BPE and intro to lemmatization" />
<meta property="og:description" content="Understanding the workings of BPE and intro to lemmatization" />
<link rel="canonical" href="https://gitlost-murali.github.io/daily-report/slp100/2021/05/17/bpe-lemma.html" />
<meta property="og:url" content="https://gitlost-murali.github.io/daily-report/slp100/2021/05/17/bpe-lemma.html" />
<meta property="og:site_name" content="Murali Manohar Daily Tracker" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-17T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gitlost-murali.github.io/daily-report/slp100/2021/05/17/bpe-lemma.html","@type":"BlogPosting","headline":"BPE algo, Lemmatization","dateModified":"2021-05-17T00:00:00-05:00","datePublished":"2021-05-17T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gitlost-murali.github.io/daily-report/slp100/2021/05/17/bpe-lemma.html"},"description":"Understanding the workings of BPE and intro to lemmatization","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/daily-report/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://gitlost-murali.github.io/daily-report/feed.xml" title="Murali Manohar | Daily Tracker" /><link rel="shortcut icon" type="image/x-icon" href="/daily-report/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/daily-report/">Murali Manohar | Daily Tracker</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/daily-report/about/">About Me</a><a class="page-link" href="/daily-report/search/">Search</a><a class="page-link" href="/daily-report/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">BPE algo,  Lemmatization</h1><p class="page-description">Understanding the workings of BPE and intro to lemmatization</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-17T00:00:00-05:00" itemprop="datePublished">
        May 17, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/daily-report/categories/#slp100">slp100</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#byte-pair-encoding--lemmatization">Byte Pair Encoding &amp; Lemmatization</a>
<ul>
<li class="toc-entry toc-h2"><a href="#workings-of-bpe">Workings of BPE</a>
<ul>
<li class="toc-entry toc-h3"><a href="#token-learner">Token learner</a></li>
<li class="toc-entry toc-h3"><a href="#token-segmenter">Token Segmenter</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#lemmatization">Lemmatization</a></li>
<li class="toc-entry toc-h2"><a href="#normalization">Normalization</a></li>
</ul>
</li>
</ul><h1 id="byte-pair-encoding--lemmatization">
<a class="anchor" href="#byte-pair-encoding--lemmatization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Byte Pair Encoding &amp; Lemmatization</h1>

<p>I. Out of the 3 tokenization methods I mentioned last time (BPE, Unigram Language Modeling &amp; WordPiece), BPE is the most common and easy to understand.
II. Lemmatization is often used in Information Extraction, Retrieval since search systems must behave similarly to morphological variants of the same word. (Cricketer, Cricket, Dinners, Dinner, etc.)</p>

<h2 id="workings-of-bpe">
<a class="anchor" href="#workings-of-bpe" aria-hidden="true"><span class="octicon octicon-link"></span></a>Workings of BPE</h2>

<p>As mentioned earlier, each tokenization scheme consists of two parts,</p>

<ol>
  <li>Token learner</li>
  <li>Token segmenter</li>
</ol>

<h3 id="token-learner">
<a class="anchor" href="#token-learner" aria-hidden="true"><span class="octicon octicon-link"></span></a>Token learner</h3>

<p>Let’s understand the first part i.e 1. Inducing vocabulary</p>

<p>BPE starts by segmenting the tokens from inside. To be specific, input given to BPE is a list of tokens split by white space and BPE learns sub-words based on the information inside each token.</p>

<p>Let’s understand this with the following example,</p>

<p>For brevity, let’s assume a tiny input corpus with few words. We split the corpus by white space and append a “_” for each token marking the word-ending. We then maintain a frequency of each word. This info is given to BPE algo.</p>

<blockquote>
  <p>Input to BPE</p>
  <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>Count   Token
5        l o w _
2        l o w e s t _
6        n e w e r _
3        w i l d e r _
2        n e w _
</code></pre></div>  </div>
  <p>We are passing the tokens as list of characters.</p>
</blockquote>

<p>We initialize the vocabulary with the set of uniq characters. As per this example, it will be</p>

<p><code class="language-plaintext highlighter-rouge">Vocabulary V -&gt; d, e, i, l, n, o, r, s, t, w</code></p>

<p>Now, we count all adjacent pairs across the training corpus and pick the one with high frequency.</p>

<p>Ex:</p>

<p>For the first word <code class="language-plaintext highlighter-rouge">low_</code>, adjacent pairs are <code class="language-plaintext highlighter-rouge">lo</code>, <code class="language-plaintext highlighter-rouge">ow</code>, <code class="language-plaintext highlighter-rouge">w_</code> and freq of each it is 5 i.e freq of the word <code class="language-plaintext highlighter-rouge">low_</code>.
If we consider all words and update the freq counts of these adjacent pairs, we’d be getting</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>er: 9
ne: 8
lo: 7
..
..
</code></pre></div></div>

<p>Since <code class="language-plaintext highlighter-rouge">er</code> has high freq, we note this merge <code class="language-plaintext highlighter-rouge">(e,r)</code> and add the pair to vocabulary. We replace all adjacent pair instances of <code class="language-plaintext highlighter-rouge">e r</code> with <code class="language-plaintext highlighter-rouge">er</code>. Updated vocabulary is</p>

<p><code class="language-plaintext highlighter-rouge">V = V + er</code></p>

<p>Updated input is</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Count   Token
5       l o w _
2       l o w e s t _
6       n e w er _
3       w i l d er _
2       n e w _
</code></pre></div></div>

<p>By repeating the same procedure, you’d find (er, _). And it goes on….</p>

<p>Finally, you’d be left with a vocabulary. <code class="language-plaintext highlighter-rouge">Vocabulary V -&gt; d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer_, low_</code>.</p>

<h3 id="token-segmenter">
<a class="anchor" href="#token-segmenter" aria-hidden="true"><span class="octicon octicon-link"></span></a>Token Segmenter</h3>

<p>Once we have learned the vocabulary, the second part -&gt; Token Segmenter. It will simply use the merges, in the order we learned, to split the test corpus accordingly.</p>

<p>Let’s understand how Token Segmenter with an example</p>

<p>Consider that <code class="language-plaintext highlighter-rouge">lower</code> is the new word in test set:</p>

<p>Now, according to the order, our first merge is (e,r), so</p>

<p>l o w e r _ -&gt; l o w <strong>er</strong> _</p>

<p>Next, we find (er,_), so</p>

<p>l o w er _ -&gt;  l o w <strong>er_</strong></p>

<p>Next, we find (l,o), so</p>

<p>l o w er_ -&gt; <strong>lo</strong> w er_</p>

<p>Next, we find (lo, w), so</p>

<p>lo w er_ -&gt; <strong>low</strong> er_</p>

<p>We are finally left with two tokens for the token <em>lower</em>, low and er_.</p>

<h2 id="lemmatization">
<a class="anchor" href="#lemmatization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lemmatization</h2>

<p>Lemmatization is the task of identifying whether two words come from the same root word. 
For ex,</p>

<p>i) <code class="language-plaintext highlighter-rouge">is, are, am</code> belong to the root <code class="language-plaintext highlighter-rouge">be</code>.</p>

<p>ii) <code class="language-plaintext highlighter-rouge">memories, memory</code> has the root word <code class="language-plaintext highlighter-rouge">memory</code>.</p>

<p>There are two ways to do lemmatization.</p>

<ol>
  <li>
    <p>Morphological Parsing (Sophisticated &amp; complex): Morphology is the study of how a word is formed by combined smaller meaning-bearer units often called Morphemes. Morphemes can be classified into two forms. a. <strong>Stems</strong> b. <strong>Affixes</strong>.
 a. Stems: Central morphemes that supply the main meaning of the word
 b. Affixes: Additional part of the word.</p>

    <p>Ex: Cars -&gt; Car + s</p>
  </li>
  <li>
    <p>Stemming (Simple yet rudimentary): Stemming logic is mostly based on chopping off the affixes of a word. Porter-Stemmer is a rule-based system where input is processed through a series of steps (cascade).</p>
  </li>
</ol>

<p>Sample rules:</p>

<p>ATIONAL -&gt; ATE (Relational -&gt; Relate)
ING -&gt; “” If Stem contains vowel (Monitoring -&gt; Monitor)
SSES -&gt; SS (Grasses -&gt; Grass, Kisses -&gt; Kiss)</p>

<p>Stemming is often prone to errors with the only advantage as being faster.</p>

<h2 id="normalization">
<a class="anchor" href="#normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalization</h2>

<p>Normalization is the process of converting words to a standard format. This is useful in Information retrieval systems. For ex: US &amp; USA represent the country United States. Wow, Wowww mean Wow.</p>

<p>Case-Folding: Lower casing the words has an upside of lowering the workload of NLP systems. For example, Boring and boring mean the same thing. Just storing <code class="language-plaintext highlighter-rouge">boring</code> would save space. However, this has exceptions.</p>
<ol>
  <li>It is difficult to differential Location <code class="language-plaintext highlighter-rouge">US</code> and Pronoun <code class="language-plaintext highlighter-rouge">us</code>, when lower-cased.</li>
  <li>True-casing helps in understanding the emotion of text. For ex, Capitalized words would magnify the emotion of reviewer in a review. These features help the classifier.
And so on..</li>
</ol>

  </div><a class="u-url" href="/daily-report/slp100/2021/05/17/bpe-lemma.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/daily-report/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/daily-report/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/daily-report/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Writing down what I read/learn everyday</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/gitlost-murali" title="gitlost-murali"><svg class="svg-icon grey"><use xlink:href="/daily-report/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/gitlostmurali" title="gitlostmurali"><svg class="svg-icon grey"><use xlink:href="/daily-report/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
