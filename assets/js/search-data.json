{
  
    
        "post0": {
            "title": "Tokenization",
            "content": "Tokenization . Cases to handle while tokenizing . Instead of a formal definition, let’s understand what is needed from a tokenizer from the following examples: . Punctuation based sentence tokenization: It is a general pattern to split text based on “(.) period” and consider the chunks as sentences. However that logic fails in the following instances: . Currency : $ 55,000.98 | Names: K.L. John | Website urls: http://google.com | Abbreviations: C.D.C/M.S | Entity Recognition . A good tokenizer should recognize New York, Walkie-Talkie as single tokens. This tells us that tokenization is inherently tied-up with Entity Recognition. . Clitic . . Note: Clitic: Part of a word that can&#39;t stand on its own &amp; can only occur when attached with other words. Ex: `&#39;re` in You&#39;re A good tokenizer must be able convert a clitic back to a meaningful word. For example: Tokenizing you&#39;re to two tokens you, are. . Dealing with Ambiguities . Apostrophe usage in different cases: . Genitive marker -&gt; Book’s cover | Qoutative marker -&gt; ‘Alright, move now’, she said. | Clitics -&gt; “They’re” | Complexities in different languages . Languages like Japanese, Thai and Chinese don’t use space to mark word boundaries. Especially, in Chinese language, a token is made up of characters which themselves carry a meaning (Morpheme). Because of this property, a sentence can be perceived differently by tokenizing it differently. Also, it becomes increasingly difficult to maintain a vocabulary when different character combinations are possible. Therefore, most NLP applications use Character-Based tokenization. . . Note: Morpheme: Morpheme is a smallest unit of a word that carries meaning. Ex: Incoming -&gt; In, come, -ing . . Note: It is important to note that Morpheme sometimes doesn&#39;t exist alone compared to word which always exists alone. Subword Tokenization . As the list gets exhaustive, it becomes increasingly tough to build a tokenizer that handles all such cases. And since tokenization is just the first step of an NLP application, it must be fast. . Considering all the situations above, research fraternity is increasingly adopting a third-way (apart from word/char based tokenization) i.e Sub-Word Segmentation. . A Subword tokenizer statistically fragments a token into subtokens, which may or may not make sense. This helps in handling new words in inference. If training set only had low and system received the word lower, it is helpful to break it into low + er. . Tokenization schemes . Most subword tokenization schemes usually have two parts, . Token learner : Induces a vocabulary based on the training corpus. | Token Segmenter: Splits a sentence into tokens in the learned vocabulary. | BPE, Unigram Language Modeling and WordPiece are some of the widely adopted tokenization methods. . Let’s understand the algo behind BPE (how a vocabulary is induced) tomorrow. .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2020/05/16/tokenization.html",
            "relUrl": "/slp100/2020/05/16/tokenization.html",
            "date": " • May 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gitlost-murali.github.io/daily-report/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gitlost-murali.github.io/daily-report/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}