{
  
    
        "post0": {
            "title": "Minimum Edit Distance & Backtracing alignment",
            "content": "Edit Distance . String Similarity . Much of NLP use-cases spectrum require similarity b/w two strings. This can be considered at two levels, . Character level (Spelling Correction): User entered word graffe would probably mean giraffe rather than grail or coffee by assuming that giraffe is just a character away. | Word Level (Coreference Resolution): Strings Hollywood director James and Hollywood movie director James refer to the same entity. The fact that these differ only by a single word can be used as evidence to decide if these strings are coherent or not. | Edit Distance . Edit Distance provides a way to quantify the assumptions on string similarities. Specifically, minimum edit distance is the minimum number of operations needed to transform one string to another. Edit distance is inversely proportional to the similarity of strings. . Cost of operations: To transform one word to another, three operations are used. Namely, insert, delete, substitute. We can assign different costs to each operation. Levenstein, in his alternate version, suggested removing substitute operation, thereby meaning substitute = delete + insert. If insert, delete have a cost 1, substitute would have a cost 2 for execution. . Solution Space: Solution space to transform one word to another is huge. We don’t want to exhaust our compute power iterating over all possibilities. Moreover, it is possible, two paths might have the same outcome despite diff paths. Therefore, Dynamic programming is used to find optimal path and edit distance. . Computing Minimum Edit Distance . Dynamic programming is the concept of solving large problems by combining solutions of sub-problems. . References: . Backtracing alignments -&gt; https://www.youtube.com/watch?v=WBcX8pbHAP4 Computing Minimum Edit Distance -&gt; https://www.youtube.com/watch?v=kgcEaoM_QJA .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2020/05/18/edit-dist.html",
            "relUrl": "/slp100/2020/05/18/edit-dist.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "BPE algo,  Lemmatization",
            "content": "Byte Pair Encoding &amp; Lemmatization . I. Out of the 3 tokenization methods I mentioned last time (BPE, Unigram Language Modeling &amp; WordPiece), BPE is the most common and easy to understand. II. Lemmatization is often used in Information Extraction, Retrieval since search systems must behave similarly to morphological variants of the same word. (Cricketer, Cricket, Dinners, Dinner, etc.) . Workings of BPE . As mentioned earlier, each tokenization scheme consists of two parts, . Token learner | Token segmenter | Token learner . Let’s understand the first part i.e 1. Inducing vocabulary . BPE starts by segmenting the tokens from inside. To be specific, input given to BPE is a list of tokens split by white space and BPE learns sub-words based on the information inside each token. . Let’s understand this with the following example, . For brevity, let’s assume a tiny input corpus with few words. We split the corpus by white space and append a “_” for each token marking the word-ending. We then maintain a frequency of each word. This info is given to BPE algo. . Input to BPE . Count Token 5 l o w _ 2 l o w e s t _ 6 n e w e r _ 3 w i l d e r _ 2 n e w _ . We are passing the tokens as list of characters. . We initialize the vocabulary with the set of uniq characters. As per this example, it will be . Vocabulary V -&gt; d, e, i, l, n, o, r, s, t, w . Now, we count all adjacent pairs across the training corpus and pick the one with high frequency. . Ex: . For the first word low_, adjacent pairs are lo, ow, w_ and freq of each it is 5 i.e freq of the word low_. If we consider all words and update the freq counts of these adjacent pairs, we’d be getting . er: 9 ne: 8 lo: 7 .. .. . Since er has high freq, we note this merge (e,r) and add the pair to vocabulary. We replace all adjacent pair instances of e r with er. Updated vocabulary is . V = V + er . Updated input is . Count Token 5 l o w _ 2 l o w e s t _ 6 n e w er _ 3 w i l d er _ 2 n e w _ . By repeating the same procedure, you’d find (er, _). And it goes on…. . Finally, you’d be left with a vocabulary. Vocabulary V -&gt; d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer_, low_. . Token Segmenter . Once we have learned the vocabulary, the second part -&gt; Token Segmenter. It will simply use the merges, in the order we learned, to split the test corpus accordingly. . Let’s understand how Token Segmenter with an example . Consider that lower is the new word in test set: . Now, according to the order, our first merge is (e,r), so . l o w e r _ -&gt; l o w er _ . Next, we find (er,_), so . l o w er _ -&gt; l o w er_ . Next, we find (l,o), so . l o w er_ -&gt; lo w er_ . Next, we find (lo, w), so . lo w er_ -&gt; low er_ . We are finally left with two tokens for the token lower, low and er_. . Lemmatization . Lemmatization is the task of identifying whether two words come from the same root word. For ex, . i) is, are, am belong to the root be. . ii) memories, memory has the root word memory. . There are two ways to do lemmatization. . Morphological Parsing (Sophisticated &amp; complex): Morphology is the study of how a word is formed by combined smaller meaning-bearer units often called Morphemes. Morphemes can be classified into two forms. a. Stems b. Affixes. a. Stems: Central morphemes that supply the main meaning of the word b. Affixes: Additional part of the word. . Ex: Cars -&gt; Car + s . | Stemming (Simple yet rudimentary): Stemming logic is mostly based on chopping off the affixes of a word. Porter-Stemmer is a rule-based system where input is processed through a series of steps (cascade). . | Sample rules: . ATIONAL -&gt; ATE (Relational -&gt; Relate) ING -&gt; “” If Stem contains vowel (Monitoring -&gt; Monitor) SSES -&gt; SS (Grasses -&gt; Grass, Kisses -&gt; Kiss) . Stemming is often prone to errors with the only advantage as being faster. . Normalization . Normalization is the process of converting words to a standard format. This is useful in Information retrieval systems. For ex: US &amp; USA represent the country United States. Wow, Wowww mean Wow. . Case-Folding: Lower casing the words has an upside of lowering the workload of NLP systems. For example, Boring and boring mean the same thing. Just storing boring would save space. However, this has exceptions. . It is difficult to differential Location US and Pronoun us, when lower-cased. | True-casing helps in understanding the emotion of text. For ex, Capitalized words would magnify the emotion of reviewer in a review. These features help the classifier. And so on.. |",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2020/05/17/bpe-lemma.html",
            "relUrl": "/slp100/2020/05/17/bpe-lemma.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Tokenization",
            "content": "Tokenization . Cases to handle while tokenizing . Instead of a formal definition, let’s understand what is needed from a tokenizer from the following examples: . Punctuation based sentence tokenization: It is a general pattern to split text based on “(.) period” and consider the chunks as sentences. However that logic fails in the following instances: . Currency : $ 55,000.98 | Names: K.L. John | Website urls: http://google.com | Abbreviations: C.D.C/M.S | Entity Recognition . A good tokenizer should recognize New York, Walkie-Talkie as single tokens. This tells us that tokenization is inherently tied-up with Entity Recognition. . Clitic . . Note: Clitic: Part of a word that can&#39;t stand on its own &amp; can only occur when attached with other words. Ex: `&#39;re` in You&#39;re A good tokenizer must be able convert a clitic back to a meaningful word. For example: Tokenizing you&#39;re to two tokens you, are. . Dealing with Ambiguities . Apostrophe usage in different cases: . Genitive marker -&gt; Book’s cover | Qoutative marker -&gt; ‘Alright, move now’, she said. | Clitics -&gt; “They’re” | Complexities in different languages . Languages like Japanese, Thai and Chinese don’t use space to mark word boundaries. Especially, in Chinese language, a token is made up of characters which themselves carry a meaning (Morpheme). Because of this property, a sentence can be perceived differently by tokenizing it differently. Also, it becomes increasingly difficult to maintain a vocabulary when different character combinations are possible. Therefore, most NLP applications use Character-Based tokenization. . . Note: Morpheme: Morpheme is a smallest unit of a word that carries meaning. Ex: Incoming -&gt; In, come, -ing . . Note: It is important to note that Morpheme sometimes doesn&#39;t exist alone compared to word which always exists alone. Subword Tokenization . As the list gets exhaustive, it becomes increasingly tough to build a tokenizer that handles all such cases. And since tokenization is just the first step of an NLP application, it must be fast. . Considering all the situations above, research fraternity is increasingly adopting a third-way (apart from word/char based tokenization) i.e Sub-Word Segmentation. . A Subword tokenizer statistically fragments a token into subtokens, which may or may not make sense. This helps in handling new words in inference. If training set only had low and system received the word lower, it is helpful to break it into low + er. . Tokenization schemes . Most subword tokenization schemes usually have two parts, . Token learner : Induces a vocabulary based on the training corpus. | Token Segmenter: Splits a sentence into tokens in the learned vocabulary. | BPE, Unigram Language Modeling and WordPiece are some of the widely adopted tokenization methods. . Let’s understand the algo behind BPE (how a vocabulary is induced) tomorrow. .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2020/05/16/tokenization.html",
            "relUrl": "/slp100/2020/05/16/tokenization.html",
            "date": " • May 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gitlost-murali.github.io/daily-report/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gitlost-murali.github.io/daily-report/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}