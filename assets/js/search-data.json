{
  
    
        "post0": {
            "title": "Smoothing",
            "content": "Smoothing . LM fails in test set when a known word occurs in an unknown context. We will discuss how to handle this issue in this blog. . . Note: If any of it sounds alien, refer to the previous blog. If any word has a zero probability in the test set, then probability of entire set becomes zero. To avoid this, smoothing algoritms shave off the probability mass from high frequency words and give it to the zero probability ones. . Laplace smoothing . This is the most straightforward approach where add 1 to all bigram counts. Laplace smoothing is only used as baselines but not employed in any modern LM systems. .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/06/09/smoothing.html",
            "relUrl": "/slp100/2021/06/09/smoothing.html",
            "date": " • Jun 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Generalization and Zeroes",
            "content": "Generalization and Zeroes . Perplexity vs Coherence vs Overfitting vs N-Gram size . It is a known fact that Statistical models are heavily dependent on the training corpus. These models encode specific facts about the training corpus. An implication of this of could be overfitting. Before we talk about overfitting, let’s go through an exercise to understand how N-gram size impacts the coherence of sentences generated. . The exercise is to generate sentences from different n-gram models by randomly generating each word. . Coherence Test . In case of Uni-gram, we randomly generate each word. To perform this random sampling, we first generate a space (0 to 1) filled with words whose interval is proportional to their frequencies. Next, we randomly sample a number b/w 0-1 and pick the word whose interval has this generated number. We continue these steps until &lt; /s&gt; is generated. . Sort the words based on frequency | Next, for each word, divide its frequency with total counts to get the percentage of space occupied by this word. This way, we generate intervals for each word. For more clarity, look at the example below. | Next, we randomly sample a number b/w 0-1 and pick the word whose interval has this generated number. | For example, if you have 18000 total counts and k words. . After sorting, let’s say our top word (say $w_{1}$) has 9000 counts and next word ($w_{2}$) has 180 counts and so on… | Now for $w_{1}$, percentage = 9000/18000 = 0.5. So, the interval for this word is 0 - 0.5 | For $w_{2}$, percentage = 180/18000 = 0.01. So, the interval for this word is 0.5 - 0.51 …. | Once we have the intervals, if the randomly generated number is 0.43. Since 0.43 lies in the interval of $w_{1}$ (0-0.5), $w_{1}$ is randomly selected word. . Incase of bigram, we start with &lt;s&gt; and pick one bigram (randomly) out of all the bigrams that start with &lt;s&gt;. We can use the same technique (used for unigram earlier) for randomly sampling an item. Say, the second word is w. Now, we look for bigrams that start with w. And so on. . When we generate sentences with these n-gram models, it is found that, . Larger the value of N in N-Gram, greater the coherence of generated sentences. . 4-gram sentences are more coherent and look more like Shakespeare. If we look at the corpus statistics, N = 884647 &amp; V = 29006, possible bigrams are $V^{2}$ and 4-grams are $V^{4}$. There’s no way that all the 4-grams are shakespearean. If we look closely, N-Gram table will be mostly sparse. So, if our 1st 4-gram is It cannot be but, there are only few possible combinations. In most cases, this also means a single continuation. This is the reason why what’s looking like Shakespeare are actually Shakespeare sentences. . If we compare the sentences generated from N-grams trained on different domains, say Shakespeare and News domain, there will be little overlap b/w them. This points to the fact that statistical models are too dependent on the training corpus and useless for inference in a different domain. So, we must always ensure the training and inference set must have same genre and dailect too. . Despite getting the same genre &amp; dailect, models are still subject to the sparsity problem. Any n-gram/phrase that has occured many times in the training set will have a good probability estimate when occured in the test set. However, it is not possible for a corpus to encode all possible words or variations of phrases. Because of this, n-grams that should have non-zero probability will have zero-probability assigned to them. . For example, training set has following details . denied the rumours 5 denied the speculations 2 . In test set, if we have the following phrases . denied the loan denied the offer . Model will incorrectly estimate that P(load|denied the) is 0!. . This leads to two problems. . This undermines the generalizability and usage of model in real-life applications. | If the probability of any word is 0, then probability of entire set is 0 (remember chain rule). | In the next post, we’ll discuss how to handle the issue of unseen n-grams through Smoothing. . Unknown words . In the age of Social media, lingo is always evolving adding more words to the vocabulary. It is possible that a language model trained on one corpus can come across Out-of-Vocabulary(OOV) words in the test set. These words are often converted into &lt;UNK&gt; tokens. There are two ways to train probabilities for &lt;UNK&gt; token, . Come up with a pre-built lexicon or words and mask words that are not in the set with &lt;UNK&gt;. | Create the lexicon from training corpus (the usual way) and then mask words that have $freq&lt;k$ as . | After generating &lt;UNK&gt;s in training corpus in either way (Step 1 or 2), we can start training the LM. | The choice of &lt;UNK&gt; assigning process has an impact on the metrics like Perplexity, If we have a small vocabulary, model can achieve less perplexity by predicting everything as &lt;UNK&gt; which anyway has higher probability. Note that, two LMs can only be compared when they have same vocabulary. .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/06/08/generalization-zeros.html",
            "relUrl": "/slp100/2021/06/08/generalization-zeros.html",
            "date": " • Jun 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Evaluating Language Models",
            "content": "Evaluating Language Model . Extrinsic and Intrinsic evaluation . One way to check the performance of Language Model is to embed it in a application and check the performance improvement of the application. This end-end evaluation of the Language Model is called Extrinsic evaluation. For instance, in case of Speech recognition, we can run it twice with both LMs and compare which LM is giving better transcription. Intuitively, this evaluation makes more sense since we can know if the particular component is improving the application or not. However, running systems end-end for evaluating LMs is costly and not recommended. . . Note: Other applications could be Spell correction system or a Machine Translation system. Instead, we can check the quality of the Language Model independent of any application. This is often referred to as Intrinsic Evaluation. After dividing the corpus into train and test splits, we train two different N-Gram models on the training data and check which LM returns high probability for the test sentences. This evaluation with the test set is called intrinsic evaluation. . . Note: A good language model assigns higher probability to real or frequently observed sentences. Perplexity . Perplexity is a quantifiable metric that is used for intrinsic evaluation. Mathematically, it is the inverted probability of test set normalized by the length N (no of words). . PP(W)=P(w1...wN)−1/NPP(W) = P(w_{1}...w_{N})^{-1/N}PP(W)=P(w1​...wN​)−1/N . Higher the probability of a test sentence, lower will be the perplexity considering the inverse. In other terms, maximizing probability would mean lowering perplexity. . Intuitively, Perplexity can be thought of as weighted average branching factor of the language. The branching factor of a language is the no.of possible next words that can follow any word. So, in simple terms, Perplexity tells us, on an average, how many words can be expected for the next word. . Understanding it through an example, . Instance-1: . If you have 10 words in a toy language and all the words have equal probability of occuring (in train and test set), then for a test sentence of length N, Perplexity = 10. . Here’s how, . PP(W) = P(w1...wN)−1/NPP(W) = ((1/10)N)−1/NPP(W) = 10PP(W) = P(w_{1}...w_{N})^{-1/N} newline PP(W) = ((1/10)^{N})^{-1/N} newline PP(W) = 10PP(W) = P(w1​...wN​)−1/NPP(W) = ((1/10)N)−1/NPP(W) = 10 . . Note: We are considering unigram probabilities here. Instance-2 . Let’s take another instance where the probabilities are different, say, 1st word has 91 occurences and others occur 1 time each. Now, perplexity would be low for a test sentence with all 1st word s. In this case, Perplexity will be low. . Note that, in instance 1 &amp; 2, branching factor is still 10. However, perplexity is different. .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/06/07/evaluating-lms.html",
            "relUrl": "/slp100/2021/06/07/evaluating-lms.html",
            "date": " • Jun 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Language Models - N-Grams & Markov assumptions & Maximum Likelihood",
            "content": "Language Models . Briefly, Language Models, are models that assign probability to each possible next word given a sequence. LMs can also assign the probability to sentences. This helps in choosing which of the two sentences are more probable in occuring in the corpus. . LMs are used in grammar correction and speech recognition systems to select the appropriate phrase or sequence. Statistical LMs make use of N-grams for obtaining probabilities. . N-Grams . N-Grams are sequence of n-words. Bi-grams, Tri-grams, etc. . Predicting probability of next word . One way to predict the probability of next word is in the following way, . P(w/h)=P(w∩h)/P(h)P(w/h) = P(w ∩ h)/ P(h)P(w/h)=P(w∩h)/P(h) . P(the∣BS is so great that)=#(BS is so great that the)/#(BS is so great that)P(the| BS is so great that) = #(BS is so great that the)/ #(BS is so great that)P(the∣BS is so great that)=#(BS is so great that the)/#(BS is so great that) . Here, we are using relative counts which are pre-calculated from the corpus/web. This method fails even for a slight change in the sentence. . For ex, a new sentence The Mombie is so great that may contain a new lingo like Mombie. Since it’s a new word, the occurence of #(The Mombie is so great that) would be 0. . Predicting probability of a sentence . Similarly, to calculate the probability of a sentence or a joint probability of sequence of words like the country is so naive, we could do, . #(the country is so naive)/#(All possible five word order sequences) #(the country is so naive) / #(All possible five word order sequences)#(the country is so naive)/#(All possible five word order sequences) . Calculating the denominator i.e #(All possible five word order sequences) is a computational heavy process and is not recommended. . Instead, we could leverage chain rule of probability to solve this issue where, . P(X1,X2,X3,...Xn)=P(X1) x P(X2∣X1) x P(X3∣X12) x ...P(Xn∣X1n−1)P(X_{1}, X_{2}, X_{3}, ... X_{n}) = P(X_{1}) x P(X_{2}|X_{1}) x P(X_{3}|X_{1}^{2}) x ... P(X_{n}|X_{1}^{n-1})P(X1​,X2​,X3​,...Xn​)=P(X1​) x P(X2​∣X1​) x P(X3​∣X12​) x ...P(Xn​∣X1n−1​) . We can apply the same to words.. . P(w1,w2,w3,...wn)=P(w1) x P(w2∣w1) x P(w3∣w12) x ...P(wn∣w1n−1)P(w_{1}, w_{2}, w_{3}, ... w_{n}) = P(w_{1}) x P(w_{2}|w_{1}) x P(w_{3}|w_{1}^{2}) x ... P(w_{n}|w_{1}^{n-1})P(w1​,w2​,w3​,...wn​)=P(w1​) x P(w2​∣w1​) x P(w3​∣w12​) x ...P(wn​∣w1n−1​) . Where w1n−1w_{1}^{n-1}w1n−1​ means the string w1w1..wn−1w_{1} w_{1} .. w_{n-1}w1​w1​..wn−1​ . A sore thumb in this approach is calculating the probability P(wn∣w1n−1)P(w_{n}|w_{1}^{n-1})P(wn​∣w1n−1​) since it is difficult to estimate it by counting the #(word occurs with its history), as language is creative (discussed above too - Mombie). . Markov assumption . So, we approximate the long previous history with last k words. Specifically, instead of calculating P(word given entire history), we just calculate P(word given last k words). This is the ideology behing N-gram model. . Whenever we use bigram model to make a prediction, we make the following assumption, . P(wn∣w1n−1)≈P(wn∣wn−1)P(w_{n}|w_{1}^{n-1}) approx P(w_{n}|w_{n-1})P(wn​∣w1n−1​)≈P(wn​∣wn−1​) . The above assumption is called Markov assumption. Markov models are a class of probabilistic models that make an assumption that we can predict the probability of a future event w/o looking too far into the past. . Maximum Likelihood estimate . Probability P(word) can be estimated by Maximum Likelihood estimate. Broadly, Estimation of parameters from the existing data points is called Maximum Likelihood Estimation. In terms of distribution, the definition can modified to Estimation of optimal mean, variance (parameters in this case) from the existing data points. In terms of text, the definition would be estimation of probability from the existing corpus. . As per SLP book, . We get MLE estimate for the parameters of an N-Gram model by getting counts from a corpus and normalizing the counts so that they lie between 0 and 1. For probabilistic models, normalizing means dividing by the total count so that probabilities always lie in the range of 0,1. . . Note: For better understanding of Maximum Likelihood, visit [StatQuest&#39;s video](https://www.youtube.com/watch?v=XepXtl9YKwc) MLE estimate for a Bi-Gram model, P(wn∣w1n−1)=#(wn−1 wn)/#(wn−1)P(w_{n}|w_{1}^{n-1}) = #(w_{n-1} w_{n})/ #(w_{n-1})P(wn​∣w1n−1​)=#(wn−1​ wn​)/#(wn−1​) . Incase of a N-gram, MLE estimate is . P(wn∣w1n−1)=#(wn−N+1...wn−1 wn)/#(wn−N+1..wn−1)P(w_{n}|w_{1}^{n-1}) = #(w_{n-N+1} ... w_{n-1} w_{n})/ #(w_{n-N+1}..w_{n-1})P(wn​∣w1n−1​)=#(wn−N+1​...wn−1​ wn​)/#(wn−N+1​..wn−1​) . More on Maximum Likelihood Estimate . Well, MLE is more of a daunting word for beginners. What we are essentially doing is a simple relative frequency =&gt; Out of some total counts, how many are suitable counts. That’s what we are doing. . Utilizing this relative frequency as an estimate for probability is an example of Maximum Likelihood estimation. Whatever method you chose, the resulting optimal parameter set found for model M should maximize the likelihood of the training set T given M i.e $ P(T/M) $. We can think of it in different domains. For deep learning folks, this means whatever architecture you chose, your accuracy must be high for the training set. . Concepts in practice . In this section, let’s see how we can calculate the probability of a sentence. . Given a corpus, we can generate bigrams and their frequencies. For a sample corpus, these are the statistics: . . i | want | to | eat | chinese | food | lunch | spend | . 2533 | 927 | 2417 | 746 | 158 | 1093 | 341 | 278 | . Frequencies of each word. Call them unigrams too. Now, let’s normalize the counts which can then be directly used in MLE estimate. . . Let’s calculate the probability of a sentence with the above counts . I want english food . P(&lt;s&gt; I want english food &lt;/s&gt;)=P(I∣&lt;s&gt;)∗P(want∣I)∗P(english∣want)∗P(food∣want)∗P(&lt;/s&gt;∣food)P(&lt;s&gt; I want english food &lt;/s&gt;)=0.0031P(&lt;s&gt; I want english food &lt;/s&gt;) = P(I|&lt;s&gt;) * P(want|I) * P(english|want) * P(food|want) * P(&lt;/s&gt;|food) newline newline . P(&lt;s&gt; I want english food &lt;/s&gt;) = 0.0031&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;P(&lt;s&gt; I want english food &lt;/s&gt;)=P(I∣&lt;s&gt;)∗P(want∣I)∗P(english∣want)∗P(food∣want)∗P(&lt;/s&gt;∣food)P(&lt;s&gt; I want english food &lt;/s&gt;)=0.0031&lt;/span&gt;&lt;/span&gt; .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/06/06/ngram-lm.html",
            "relUrl": "/slp100/2021/06/06/ngram-lm.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Minimum Edit Distance & Backtracing alignment",
            "content": "Edit Distance . String Similarity . Much of NLP use-cases spectrum require similarity b/w two strings. This can be considered at two levels, . Character level (Spelling Correction): User entered word graffe would probably mean giraffe rather than grail or coffee by assuming that giraffe is just a character away. | Word Level (Coreference Resolution): Strings Hollywood director James and Hollywood movie director James refer to the same entity. The fact that these differ only by a single word can be used as evidence to decide if these strings are coherent or not. | Edit Distance . Edit Distance provides a way to quantify the assumptions on string similarities. Specifically, minimum edit distance is the minimum number of operations needed to transform one string to another. Edit distance is inversely proportional to the similarity of strings. . Cost of operations: To transform one word to another, three operations are used. Namely, insert, delete, substitute. We can assign different costs to each operation. Levenstein, in his alternate version, suggested removing substitute operation, thereby meaning substitute = delete + insert. If insert, delete have a cost 1, substitute would have a cost 2 for execution. . Solution Space: Solution space to transform one word to another is huge. Moreover, it is possible that two paths might have the same outcome despite diff paths. We don’t want to exhaust our compute power iterating over all possibilities. Therefore, Dynamic programming is used to find optimal path and edit distance. . Computing Minimum Edit Distance . Dynamic programming is a tabled-based method used to solve large problems by combining solutions of sub-problems. The intituition is that a large problem can be solved by properly combining the solutions to various sub-problems. . Using DP for Edit Distance . Given a source string X of length n and target Y of length m, when we say various sub-problems, we mean all possible conversions i.e Transforming all source sub-strings $X[0..i]$ (where $0&lt;=i&lt;=n$) to target sub-strings $Y[0..j]$ ( where $0&lt;=j&lt;=m$). D[i,j] is the edit distance b/w $X[0..i]$ &amp; $Y[0..j]$. Minimum edit distance for X,Y is D[n,m]. . Let’s consider the following case, . X = “intention” Y = “execution” . For sub-problems like $D[i,0]$, cost involved is i as there are i delete operations involved to reach the target string &quot;&quot; (null). Similarly, for $D[0,j]$, cost involved is j since it’d require j insert operations to transform an empty string into the one with j characters. . If the above made statement didn’t make sense, try it out with few samples, . Sub-Problem | Src -&gt; Tgt | Operations involved | . D[0,0] | ”” -&gt; “” | 0 | . D[0,1] | ”” -&gt; “e” | 1 insert operation | . D[0,2] | ”” -&gt; “ex” | 2 insert operations | .   |   |   | . D[0,j] |   | j insert operations | . D[1,0] | “i” -&gt; “” | 1 delete operation | . D[2,0] | “in” -&gt; “” | 2 delete operations | . … | … |   | . D[i,0] |   | i delete operations | . Next, let’s scale up to other sub-problems . For, D[1,1], i.e, &quot;i&quot; (src) $=&gt;$ &quot;e&quot; (tgt), the operations would be delete i &amp; insert e. It can also be considered in terms of 3 possible paths i.e through 3 sub-problems, . Through D[0,1], i.e, &quot;&quot; $=&gt;$ &quot;e&quot;, where we can add a insert operation &quot;i&quot; to src side and we will arrive at D[1,1]. | Through D[1,0], i.e, &quot;i&quot; $=&gt;$ &quot;&quot;, where we can add a insert operation &quot;e&quot; to tgt side and we will arrive at D[1,1]. | Through D[0,0], i.e, &quot;&quot; $=&gt;$ &quot;&quot;, where we add two insert operations &quot;i&quot; to src and &quot;e&quot; to tgt side and we will arrive at D[1,1]. | When you repeat this several times, a common pattern emerges: . D[i,j]=min(D[i−1,j]+1,D[i,j−1]+1,D[i−1,j−1]+D[i,j] = min(D[i-1, j]+1, D[i, j-1]+1, D[i-1, j-1]+D[i,j]=min(D[i−1,j]+1,D[i,j−1]+1,D[i−1,j−1]+ (2 if source[i]!=target[j] else 0)) . Using the above logic, we can get the Minimum Edit Distance for Source &amp; Target. . . Alignments . Minimum Edit Distance alone is not much useful. Alignments i.e which character in source word corresponds to which char in target words, are useful in applications like Machine Translation, Speech Translation, etc. . We can get the alignments by simply storing which neighboring sub-problem (refer to the logic above) had the minimum cost for each sub-problem. . References: . Backtracing alignments | Computing Minimum Edit Distance |",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/05/18/edit-dist.html",
            "relUrl": "/slp100/2021/05/18/edit-dist.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "BPE algo,  Lemmatization",
            "content": "Byte Pair Encoding &amp; Lemmatization . I. Out of the 3 tokenization methods I mentioned last time (BPE, Unigram Language Modeling &amp; WordPiece), BPE is the most common and easy to understand. II. Lemmatization is often used in Information Extraction, Retrieval since search systems must behave similarly to morphological variants of the same word. (Cricketer, Cricket, Dinners, Dinner, etc.) . Workings of BPE . As mentioned earlier, each tokenization scheme consists of two parts, . Token learner | Token segmenter | Token learner . Let’s understand the first part i.e 1. Inducing vocabulary . BPE starts by segmenting the tokens from inside. To be specific, input given to BPE is a list of tokens split by white space and BPE learns sub-words based on the information inside each token. . Let’s understand this with the following example, . For brevity, let’s assume a tiny input corpus with few words. We split the corpus by white space and append a “_” for each token marking the word-ending. We then maintain a frequency of each word. This info is given to BPE algo. . Input to BPE . Count Token 5 l o w _ 2 l o w e s t _ 6 n e w e r _ 3 w i l d e r _ 2 n e w _ . We are passing the tokens as list of characters. . We initialize the vocabulary with the set of uniq characters. As per this example, it will be . Vocabulary V -&gt; d, e, i, l, n, o, r, s, t, w . Now, we count all adjacent pairs across the training corpus and pick the one with high frequency. . Ex: . For the first word low_, adjacent pairs are lo, ow, w_ and freq of each it is 5 i.e freq of the word low_. If we consider all words and update the freq counts of these adjacent pairs, we’d be getting . er: 9 ne: 8 lo: 7 .. .. . Since er has high freq, we note this merge (e,r) and add the pair to vocabulary. We replace all adjacent pair instances of e r with er. Updated vocabulary is . V = V + er . Updated input is . Count Token 5 l o w _ 2 l o w e s t _ 6 n e w er _ 3 w i l d er _ 2 n e w _ . By repeating the same procedure, you’d find (er, _). And it goes on…. . Finally, you’d be left with a vocabulary. Vocabulary V -&gt; d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer_, low_. . Token Segmenter . Once we have learned the vocabulary, the second part -&gt; Token Segmenter. It will simply use the merges, in the order we learned, to split the test corpus accordingly. . Let’s understand how Token Segmenter with an example . Consider that lower is the new word in test set: . Now, according to the order, our first merge is (e,r), so . l o w e r _ -&gt; l o w er _ . Next, we find (er,_), so . l o w er _ -&gt; l o w er_ . Next, we find (l,o), so . l o w er_ -&gt; lo w er_ . Next, we find (lo, w), so . lo w er_ -&gt; low er_ . We are finally left with two tokens for the token lower, low and er_. . Lemmatization . Lemmatization is the task of identifying whether two words come from the same root word. For ex, . i) is, are, am belong to the root be. . ii) memories, memory has the root word memory. . There are two ways to do lemmatization. . Morphological Parsing (Sophisticated &amp; complex): Morphology is the study of how a word is formed by combined smaller meaning-bearer units often called Morphemes. Morphemes can be classified into two forms. a. Stems b. Affixes. a. Stems: Central morphemes that supply the main meaning of the word b. Affixes: Additional part of the word. . Ex: Cars -&gt; Car + s . | Stemming (Simple yet rudimentary): Stemming logic is mostly based on chopping off the affixes of a word. Porter-Stemmer is a rule-based system where input is processed through a series of steps (cascade). . | Sample rules: . ATIONAL -&gt; ATE (Relational -&gt; Relate) ING -&gt; “” If Stem contains vowel (Monitoring -&gt; Monitor) SSES -&gt; SS (Grasses -&gt; Grass, Kisses -&gt; Kiss) . Stemming is often prone to errors with the only advantage as being faster. . Normalization . Normalization is the process of converting words to a standard format. This is useful in Information retrieval systems. For ex: US &amp; USA represent the country United States. Wow, Wowww mean Wow. . Case-Folding: Lower casing the words has an upside of lowering the workload of NLP systems. For example, Boring and boring mean the same thing. Just storing boring would save space. However, this has exceptions. . It is difficult to differential Location US and Pronoun us, when lower-cased. | True-casing helps in understanding the emotion of text. For ex, Capitalized words would magnify the emotion of reviewer in a review. These features help the classifier. And so on.. |",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/05/17/bpe-lemma.html",
            "relUrl": "/slp100/2021/05/17/bpe-lemma.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Tokenization",
            "content": "Tokenization . Cases to handle while tokenizing . Instead of a formal definition, let’s understand what is needed from a tokenizer from the following examples: . Punctuation based sentence tokenization: It is a general pattern to split text based on “(.) period” and consider the chunks as sentences. However that logic fails in the following instances: . Currency : $ 55,000.98 | Names: K.L. John | Website urls: http://google.com | Abbreviations: C.D.C/M.S | Entity Recognition . A good tokenizer should recognize New York, Walkie-Talkie as single tokens. This tells us that tokenization is inherently tied-up with Entity Recognition. . Clitic . . Note: Clitic: Part of a word that can&#39;t stand on its own &amp; can only occur when attached with other words. Ex: `&#39;re` in You&#39;re A good tokenizer must be able convert a clitic back to a meaningful word. For example: Tokenizing you&#39;re to two tokens you, are. . Dealing with Ambiguities . Apostrophe usage in different cases: . Genitive marker -&gt; Book’s cover | Qoutative marker -&gt; ‘Alright, move now’, she said. | Clitics -&gt; “They’re” | Complexities in different languages . Languages like Japanese, Thai and Chinese don’t use space to mark word boundaries. Especially, in Chinese language, a token is made up of characters which themselves carry a meaning (Morpheme). Because of this property, a sentence can be perceived differently by tokenizing it differently. Also, it becomes increasingly difficult to maintain a vocabulary when different character combinations are possible. Therefore, most NLP applications use Character-Based tokenization. . . Note: Morpheme: Morpheme is a smallest unit of a word that carries meaning. Ex: Incoming -&gt; In, come, -ing . . Note: It is important to note that Morpheme sometimes doesn&#39;t exist alone compared to word which always exists alone. Subword Tokenization . As the list gets exhaustive, it becomes increasingly tough to build a tokenizer that handles all such cases. And since tokenization is just the first step of an NLP application, it must be fast. . Considering all the situations above, research fraternity is increasingly adopting a third-way (apart from word/char based tokenization) i.e Sub-Word Segmentation. . A Subword tokenizer statistically fragments a token into subtokens, which may or may not make sense. This helps in handling new words in inference. If training set only had low and system received the word lower, it is helpful to break it into low + er. . Tokenization schemes . Most subword tokenization schemes usually have two parts, . Token learner : Induces a vocabulary based on the training corpus. | Token Segmenter: Splits a sentence into tokens in the learned vocabulary. | BPE, Unigram Language Modeling and WordPiece are some of the widely adopted tokenization methods. . Let’s understand the algo behind BPE (how a vocabulary is induced) tomorrow. .",
            "url": "https://gitlost-murali.github.io/daily-report/slp100/2021/05/16/tokenization.html",
            "relUrl": "/slp100/2021/05/16/tokenization.html",
            "date": " • May 16, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gitlost-murali.github.io/daily-report/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gitlost-murali.github.io/daily-report/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}